{
 "cells": [
  {
   "cell_type": "code",

   "execution_count": 1,

   "execution_count": 3,

   "id": "ffa9fb31-bfab-42d1-bcf1-063ccadcf6a9",
   "metadata": {},
   "outputs": [],
   "source": [

    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ccdv/arxiv-classification\", \"default\")"

    "from datasets import load_dataset\n"

   ]
  },
  {
   "cell_type": "code",

   "execution_count": 2,
   "id": "6d63a83f-9fe3-4de0-917c-1fd49d1fc90c",

   "execution_count": 4,
   "id": "cc0f2139-dc48-4bce-b620-496161c3c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load the original dataset\n",
    "# ds = load_dataset(\"ccdv/arxiv-classification\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea05fd1-e37f-4086-b1fb-60e48e22167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset without special characters/numbers\n",
    "ds = load_dataset(\"ThanhT04/tokenized_arxiv-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0c3cf2-400e-4bbb-a9fd-3d02f07ed27a",

   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 28388\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "})"
      ]
     },

     "execution_count": 2,

     "execution_count": 6,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0410ea5-7fb1-424c-9a87-c07e76a74b35",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 3,

   "execution_count": 7,

   "id": "3d82c234-946f-455a-8eb9-2641c35bdbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds['train']\n",
    "ds_val = ds['validation']\n",
    "ds_test = ds['test']"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 4,

   "execution_count": 8,

   "id": "e883f883-0df5-43e9-a127-fdf4ff6530d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = ds_train['text']\n",
    "train_label = ds_train['label']"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 5,

   "execution_count": 9,

   "id": "b6790b23-c4c7-42d5-bd46-94b728d59c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text = ds_val['text']\n",
    "val_label = ds_val['label']"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 6,

   "execution_count": 10,

   "id": "76ee6dfb-fead-451e-b738-f36fc396cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = ds_test['text']\n",
    "test_label = ds_test['label']"
   ]
  },
  {
   "cell_type": "code",


   "execution_count": 11,
   "id": "27d7b955-43a4-456d-9b20-003ee4a440db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization to remove non-word characters\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# import re\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# def tokenize_text(list):\n",
    "#     tokenized_list = []\n",
    "#     stops = set(stopwords.words('english'))\n",
    "#     for text in list:\n",
    "#         container = nltk.word_tokenize(text)\n",
    "#         filtered_words = [word.lower() for word in container if (word.isalpha() and word.lower() not in stops) if re.match(r'^[A-Za-z-]+$', word) and len(word) >= 3]\n",
    "#         tokenized_list.append(\" \".join(filtered_words))\n",
    "#     return tokenized_list\n",
    "    \n",
    "# tokenized_val = tokenize_text(val_text)\n",
    "# tokenized_train = tokenize_text(train_text)\n",
    "# tokenized_test = tokenize_text(test_text)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 7,
   "id": "345b0789-67e7-4f96-bc98-3a939b262ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text embeddings: TFIDF\n",
    "# Calculate how relevant a word in a series or corpus is to a text"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 8,

   "execution_count": 76,

   "id": "2d02d358-703e-4ead-a8af-570d546c7e78",
   "metadata": {},
   "outputs": [],
   "source": [

    "from sklearn.feature_extraction.text import TfidfVectorizer\n"

    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000) "

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f94a5c-dcba-44f3-91f4-0ab4a3af3944",
   "metadata": {},
   "outputs": [],
   "source": [

    "vectorizer = TfidfVectorizer(max_features=5000) \n",

    "\n",

    "X_train = vectorizer.fit_transform(train_text).toarray()\n",
    "X_validation = vectorizer.transform(val_text).toarray()\n",
    "X_test = vectorizer.transform(test_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "043887ff-d8e1-4f80-bd95-64d13bc50433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "225292d7-193e-4ca1-be13-6ddf6a265d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(train_label, dtype=torch.long)\n",
    "y_validation = torch.tensor(val_label, dtype=torch.long)\n",
    "y_test = torch.tensor(test_label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10d28479-15ba-4983-ad5a-cd0fb818eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), y_train)\n",
    "val_dataset = TensorDataset(torch.tensor(X_validation, dtype=torch.float32), y_validation)\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b7d2760-783c-4bee-bb22-b47a4f675db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7bbe1e2-b482-4b2d-aeff-557a32002ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedforward neural network\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # Output probability for labels\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3)\n",
    "        self.fc4 = nn.Linear(hidden_dim_3, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.softmax(out)\n",
    "        # out = self.relu(out)\n",
    "        # out = self.dropout(out)\n",
    "        # out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5120178a-d543-4f37-8e63-69e7127aa17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_DIM_1 = 128\n",
    "HIDDEN_DIM_2 = 128\n",
    "HIDDEN_DIM_3 = 64\n",
    "OUTPUT_DIM = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "290eaf06-47f8-4f05-bc53-e1835f8de399",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNN(INPUT_DIM, HIDDEN_DIM_1, HIDDEN_DIM_2, HIDDEN_DIM_3, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "804ebf14-b038-43a9-b234-7efb179c39a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72667d96-62a7-42a8-8036-9054d26da122",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b371e1-af1c-4555-86fc-43b5caf304ba",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a50a1bb-caf9-40bc-a966-1642fe2e838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(torch.device('cuda'))\n",
    "        labels = labels.to(torch.device('cuda'))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, dim= 1)   \n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    return correct_predictions.double() / len(dataloader.dataset), total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7752957-6aa2-4c1c-81e7-149b8f83bf02",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe80ec35-e1ac-451d-9094-631d1b35c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "           \n",
    "            # Move data to GPU\n",
    "            inputs = inputs.to(torch.device('cuda'))\n",
    "            labels = labels.to(torch.device('cuda'))\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "    return correct_predictions.double() / len(dataloader.dataset), total_loss / len(dataloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cd943cf-0da1-4586-83f1-fea0b85e37c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train loss: 3.571048027360943, accuracy: 0.6268845991263915\n",
      "Validation loss: 3.4098825636942673, accuracy: 0.7772\n",
      "Epoch 2/10:\n",
      "Train loss: 3.4189527600248093, accuracy: 0.7705368465548824\n",
      "Validation loss: 3.394948827233284, accuracy: 0.7912\n",
      "Epoch 3/10:\n",
      "Train loss: 3.393264575206058, accuracy: 0.7939974637170636\n",
      "Validation loss: 3.376100965366242, accuracy: 0.8088000000000001\n",
      "Epoch 4/10:\n",
      "Train loss: 3.3743265647619545, accuracy: 0.8131957164999296\n",
      "Validation loss: 3.3717080438212985, accuracy: 0.8168000000000001\n",
      "Epoch 5/10:\n",
      "Train loss: 3.3645501935985727, accuracy: 0.822143159081302\n",
      "Validation loss: 3.3668292950672707, accuracy: 0.8188000000000001\n",
      "Epoch 6/10:\n",
      "Train loss: 3.3624255244832644, accuracy: 0.823622657460899\n",
      "Validation loss: 3.3791976840632736, accuracy: 0.8072\n",
      "Epoch 7/10:\n",
      "Train loss: 3.3567336044848806, accuracy: 0.8288361279413837\n",
      "Validation loss: 3.3631335823399247, accuracy: 0.8224\n",
      "Epoch 8/10:\n",
      "Train loss: 3.3525173363215486, accuracy: 0.8333450753839651\n",
      "Validation loss: 3.362412542294545, accuracy: 0.8224\n",
      "Epoch 9/10:\n",
      "Train loss: 3.352673361200682, accuracy: 0.8331689446244893\n",
      "Validation loss: 3.3648571998450407, accuracy: 0.8212\n",
      "Epoch 10/10:\n",
      "Train loss: 3.349804921754649, accuracy: 0.8359518106242074\n",
      "Validation loss: 3.359613749631651, accuracy: 0.8248000000000001\n"
     ]
    }
   ],
   "source": [
    "# Train in 10 epochs\n",
    "for epoch in range(10):\n",
    "    train_acc, train_loss = train_model(model, train_loader, loss_fn, optimizer)\n",
    "    val_acc, val_loss = eval_model(model, val_loader, loss_fn)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/10:\")\n",
    "    print(f\"Train loss: {train_loss}, accuracy: {train_acc}\")\n",
    "    print(f\"Validation loss: {val_loss}, accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42f59379-eb5d-4738-a1be-a41ce7f50597",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = eval_model(model, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3c21165-03af-4e6a-b3a3-cd11dcc05cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.348034948300404, accuracy: 0.8384\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test loss: {test_loss}, accuracy: {test_acc}\")"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81434589-d70c-4779-bef9-2fa6ba7e9421",
   "metadata": {},
   "outputs": [],
   "source": []


  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
